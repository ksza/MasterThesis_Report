%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

Before the early 1970s, computers were owned by large institutions like corporations, universities or government agencies. These machines, also know as \emph{Mainframes}, were costly and space consuming, most institutions owning only one piece of such equipment. Lacking any kind of intuitive user interface, Mainframes would accept jobs in the form of punched cards, an early input mechanism for data and programs into computers. Only a group of people in each institution had the knowledge on how to operate the machines and they represented the interface between everyday users and the Mainframe. Quite a bottleneck!\\

In 1971, the first commercial microprocessor was released, representing the rise of a new era: the Personal Computer (PC). The PC represented a huge step forward, offering in perspective the possibility for anyone to own a computer and to operate it using a novel input/output mechanism: graphical user interface, mouse, keyboard and other peripherals. This is the standard PC setup that we know and, after such o along time, still use. By this I want to point out that although we have came a long way, by hardware and software advancements, the human computer interaction (HCI) that is mostly used to interact with PCs is the same as envisioned a little more than 40 years ago.\\

The PCs revolution was followed by the idea of portable personal devices like laptops and dynabooks (an educational device similar to a nowadays tablet). Although novel at that time, all the fever generated by this revolution was viewed by a group of people at PARC\footnote{Palo Alto Research Center} as being ephemeral: ''[...] My colleagues and I at PARC think that the idea of a ''personal computer'' itself is misplaced, and that the vision of laptop machines, dynabooks and ''knowledge navigators'' is only a transitional step toward achieving the real potential of information technology. Such machines cannot truly make computing an integral, invisible part of the way people live their lives. Therefore we are trying to conceive a new way of thinking about computers in the world, one that takes into account the natural human environment and allows the computers themselves to vanish into the background''. \cite{weiser1991computer}. This futuristic concept envisioned at PARC which further shaped the way we see and use technology and devices is known as \emph{ubicomp}\footnote{Ubiquitous Computing}.\\

The novel vision in ubicomp can be viewed as a second major advancement in the world of technology. The first major advancement, the transition from Mainframe to PC, took the relation between people and computer from a many-to-one to a one-to-one relationship. Ubicomp envisioned a transition from one-to-one to one-to-many, where each user will own not one but many device. These device would range from PCs to all sorts of portable device, some specialized at accomplishing specific tasks and performing under various circumstances. Classical HCI was not fit for this new set-up. More advanced and more intuitive concepts were needed to make the best of these technological advances. One such emerging concept was \emph{context-aware computing}. Context is ''any information that can be used to characterize the situation of an entity, where an entity can be a person, place, or physical or computational object. We define context-awareness or context-aware computing as the use of context to provide task-relevant information and/or services to a user. Three important context-awareness behaviours are the presentation of information and services to a user, automatic execution of a service, and tagging of context to information for later retrieval''. \cite{abowd1999towards}. Context-awareness takes HCI to a whole different level. We are not thinking anymore of only sitting down in front of a computer and interacting with it using a few peripherals. Users and developers have a whole new spectrum they can explore, in a dynamic and continuously evolving technological environment. Take for example a piece of software that gives information on historical monuments. If this software is running on a PC, the user would manually search for a specific monument to retrieve the information. The same software running on a mobile device with localization capabilities (i.e. GPS), could take away all the explicit interaction by determining the user's location and if this is in near proximity of a certain historical monument, automatically display the relevant information.\\

While designing mobile pervasive computing environments, software developers are bound to work with cutting edge hardware and software technologies and to adopt new communication models for HCI. Throughout the years, in ubicomp, much of the effort has been devoted to address hardware and software challenges, but the area of designing systems in which device are distributed as such has been barely touched. Hence, with the explosion of devices in the form of mobile phones and tablets having a wide range of sensing capabilities, modeling of context-aware software became \emph{device-centric}. This means that designers of mobile context-aware systems base their design mostly on device sensory data.\\

In our everyday lives we encounter and interact with a wider range of mobile device. But the spectrum of everyday, physical objects we interact with is wider. As context modeling became device centric, physical objects have been greatly overlooked making it a challenge to design for them in a pervasive system. To address the struggling on conceptually incorporating the real world into the system design, Pederson in \cite{pederson2010towards} introduced a \emph{body-centric} modeling framework that incorporates physical and virtual objects of interest on the basis of proximity and human perception, framed in the context of an emerging ''egocentric'' interaction paradigm. ''Egocentric'' signals that it is the human body and mind of a specific human individual that acts as center of reference to which all modeling is anchored in this interaction paradigm. Part of the egocentric paradigm is the Situative Space Model (SSM): an interaction model and a design tool which captures what a specific human agent can perceive and not perceive, reach and not reach, at any given moment in time.\\

The SSM is meant to categorize objects around the agent, into a number of sets, based on it's continuously evolving state as it interacts with the surrounding environment. The sets maintained by the SSM are, as described in \cite{pederson2010towards}:
\begin{itemize}
	\item World Space. Contains all physical and virtual objects to be part of a certain model.
	\item Perception Space (PS). Objects around the agent that can be perceived at a given moment (i.e. objects currently in the visual spectrum of the user).
	\item Recognizable Set (RS). Objects in the PS within recognition distance.
	\item Examinable Set (ES). Objects in the PS that are within their examination range.
	\item Action Space (AS). Objects surrounding the agent that are currently accessible to the agent's physical actions.
	\item Selected Set (SdS). Objects being currently physically or virtually handled (i.e. touched).
	\item Manipulated Set (MS). Objects whose state is currently being acted upon by the agent.
\end{itemize}

Evaluating the SSM was a tedious work and consisted in creating a real-life setup, with digitally enhanced physical objects (various sensors to track agent proximity and actions) while using a wide range of technologies to track the agent's current situation (body position, orientation, visual spectrum etc.). This is a costly and time consuming process making further development and evaluation of the egocentric paradigm a challenge. As Pederson also stated in \cite{pederson2011situative}: ''accurate tracking of objects and mediators needed for real-time application of the SSM will probably remain a challenge for years to come''.\\

To overcome these challenges, in this Master Thesis I aim at designing and developing a simulation environment for mobile context-aware system design, focusing on the egocentric paradigm. The framework will monitor the agents context in regards to the world space and the context of physical and virtual objects.\\ 

I am designing this system for ubicomp researchers and developers designing mobile context-aware systems that change their behavior on how the body of the agent is in regards to the surrounding environment and entities (physical and virtual objects). To offer the most to this target community, I would like the source code to be accessible for anyone who is interested, to provide feedback, to further develop it or to modify it in order to fit certain individual needs. These aspects are best covered by the open-source software model.\\

Finally, I want to emphasize on the need for this simulator, relating to the need for simulators in ubicomp as identified by Reynolds, Cahill and Senart in \cite{reynolds2006requirements}: ''The use of simulation technology in ubiquitous computing is of particular importance to developers and researchers alike. Many of the required hardware technologies such as cheap reliable sensors are only reaching maturity now, and many of the application scenarios are being designed with the future in mind and well in advance of the hardware actually being available. Furthermore, many of the target scenarios do not lend themselves to onsite testing, in particular, scenarios which require deployment of large numbers of nodes or devices. In addition, simulation enables researchers to evaluate scenarios, applications, protocols and so forth without the difficulties in dealing with hardware sensors and actuators, and also offers greater flexibility since it is easy to run a set of simulations with a range of parameters''.\\

\input{Chapters/Chapter01/ProblemStatement}

\input{Chapters/Chapter01/Goal}

\input{Chapters/Chapter01/Method}

\input{Chapters/Chapter01/ThesisOverview}

