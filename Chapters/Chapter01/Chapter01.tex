%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

Before the early 1970s, computers were owned by large institutions like corporations, universities or government agencies. These machines, also know as \emph{Mainframes}, were costly and space consuming, most institutions owning only one piece of such equipment. Lacking any kind of intuitive user interface, Mainframes would accept jobs in the form of punched cards, an early input mechanism for data and programs into computers. Only a group of people in each institution had the knowledge on how to operate the machines and they represented the interface between everyday users and the Mainframe. Quite a bottleneck!\\

In 1971, the first commercial microprocessor was released, representing the rise of a new era: the Personal Computer (PC). The PC represented a huge step forward, offering in perspective the possibility for anyone to own a computer and to operate it using a novel input/output mechanism: graphical user interface, mouse, keyboard and other peripherals. This is the standard PC setup that we know and, after such a long time, still use. By this we want to point out that although we have came a long way, by hardware and software advancements, the human computer interaction (HCI) that is mostly used to interact with PCs is the same as envisioned a little more than 40 years ago.\\

The PCs revolution was followed by the idea of portable personal devices like laptops and dynabooks (an educational device similar to a nowadays tablet). Although novel at that time, all the fever generated by this revolution was viewed by a group of people at PARC\footnote{Palo Alto Research Center} as being ephemeral: ''[...] My colleagues and I at PARC think that the idea of a ''personal computer'' itself is misplaced, and that the vision of laptop machines, dynabooks and ''knowledge navigators'' is only a transitional step toward achieving the real potential of information technology. Such machines cannot truly make computing an integral, invisible part of the way people live their lives. Therefore we are trying to conceive a new way of thinking about computers in the world, one that takes into account the natural human environment and allows the computers themselves to vanish into the background''. \cite{weiser1991computer}. This futuristic concept envisioned at PARC, which further shaped the way we see and use information technology and devices, is known as Ubiquitous Computing, or''Ubicomp'' for short.\\

\section{Context Aware Systems}
The novel vision in Ubicomp can be viewed as a second major advancement in the world of information technology. The first major advancement, the transition from Mainframe to PC, took the relation between people and computer from a many-to-one to a one-to-one relationship. Ubicomp envisioned a transition from one-to-one to one-to-many, where each user will own not one but many device. These device would range from PCs to all sorts of portable device, some specialized at accomplishing specific tasks and performing under various circumstances. Classical HCI was not fit for this new set-up. More advanced and more intuitive concepts were needed to make the best of these technological advances. One such emerging concept was \emph{context-aware computing}. Context is ''any information that can be used to characterize the situation of an entity, where an entity can be a person, place, or physical or computational object. We define context-awareness or context-aware computing as the use of context to provide task-relevant information and/or services to a user. Three important context-awareness behaviours are the presentation of information and services to a user, automatic execution of a service, and tagging of context to information for later retrieval''. \cite{abowd1999towards}. Context-awareness takes HCI to a whole different level. We are not thinking anymore of only sitting down in front of a computer and interacting with it using a few peripherals. Users and developers have a whole new spectrum they can explore, in a dynamic and continuously evolving technological environment. Take for example a piece of software that gives information on historical monuments. If this software is running on a PC, the user would manually search for a specific monument to retrieve the information. The same software running on a mobile device with localization capabilities (i.e. GPS), could take away all the explicit interaction by determining the user's location and if this is in near proximity of a certain historical monument, automatically display the relevant information.\\

\section{Egocentric Interaction Paradigm}
While designing mobile pervasive computing environments, software developers are bound to work with cutting edge hardware and software technologies and to adopt new communication models for HCI. Throughout the years, in Ubicomp, much of the effort has been devoted to address hardware and software challenges, hence, with the explosion of devices in the form of mobile phones and tablets having a wide range of sensing capabilities, modelling of context-aware software became \emph{device-centric}. This way, designers of mobile context-aware systems base their design mostly on device sensory data.\\

In our everyday lives we encounter and interact with our surrounding environment through a wide range of mobile devices and physical objects. While the current device-centric modelling makes it easy to incorporate devices, incorporating physical objects into a context-aware system design, remains a challenge. When trying to integrate physical objects into the system design, the designer is bound to think in terms of existing technologies that can facilitate their integration; for example, making a system aware of a chair, a designer might think about attaching various sensors to it (pressure, motion, proximity, etc). While the device-centric modelling makes it natural to think about devices, when it comes to integrate physical objects, it limits the designer instead of offering a natural way to solve the problem. To address the struggling on conceptually incorporating the real world into the system design, Pederson in \cite{pederson2010towards} introduced a \emph{body-centric} modelling framework that incorporates physical objects, mediators (i.e. devices) and virtual objects (part of a runnign software, like a window, a stream of sound, interacted with through mediators) of interest on the basis of proximity and human perception, framed in the context of an emerging ''egocentric'' interaction paradigm. ''Egocentric'' signals that it is the human body and mind of a specific human individual that acts as centre of reference to which all modelling is anchored in this interaction paradigm. Part of the egocentric paradigm is the Situative Space Model (SSM): an interaction model and a design tool which captures what a specific human agent can perceive and not perceive, reach and not reach, at any given moment in time.\\

The SSM is meant to categorize objects around the agent, into a number of sets, based on it's continuously evolving state as it interacts with the surrounding environment. The sets maintained by the SSM are, as described in \cite{pederson2010towards}:
\begin{itemize}
	\item World Space. Contains all physical objects, mediators and virtual objects to be part of a certain model.
	\item Perception Space (PS). Objects around the agent that can be perceived at a given moment (i.e. objects currently in the users field of vision).
	\item Recognizable Set (RS). Objects in the PS within recognition distance.
	\item Examinable Set (ES). Objects in the PS that are within their examination range.
	\item Action Space (AS). Objects surrounding the agent that are currently accessible to the agent's physical actions.
	\item Selected Set (SdS). Objects being currently physically or virtually handled (i.e. touched).
	\item Manipulated Set (MS). Objects whose state is currently being acted upon by the agent.
\end{itemize}

Evaluating the SSM was a tedious work and consisted in creating a real-life setup, with digitally enhanced physical objects (various sensors to track agent proximity and actions) while using a wide range of technologies to track the agent's current situation (body position, orientation, visual spectrum etc.). This is a costly and time consuming process making further development and evaluation of the egocentric paradigm a challenge. As Pederson also stated in \cite{pederson2011situative}: ''accurate tracking of objects and mediators needed for real-time application of the SSM will probably remain a challenge for years to come''.\\

% in this Master Thesis we aim in this at designing a simulation environment for mobile context-aware system design, focusing on the egocentric paradigm.
In order to simplify further research into mobile context-aware systems based on the egocentric paradigm, in this Master Thesis we aim in this at designing a simulation environment for mobile context-aware system design. The framework will monitor the agent's context in regards to the world space and the context of surrounding entities. Based on the design, we will implement the \emph{EgoSim} framework\footnote{The source code is included with this thesis in the \emph{EgoSim\_SourceCode.zip} archive. It is also available on GitHub \cite{egosim:online}}. To emphasize on the need for this simulation framework, we relate to the need for simulators in Ubicomp as identified by Reynolds, Cahill and Senart in \cite{reynolds2006requirements}: ''The use of simulation technology in ubiquitous computing is of particular importance to developers and researchers alike. Many of the required hardware technologies such as cheap reliable sensors are only reaching maturity now, and many of the application scenarios are being designed with the future in mind and well in advance of the hardware actually being available. Furthermore, many of the target scenarios do not lend themselves to onsite testing, in particular, scenarios which require deployment of large numbers of nodes or devices. In addition, simulation enables researchers to evaluate scenarios, applications, protocols and so forth without the difficulties in dealing with hardware sensors and actuators, and also offers greater flexibility since it is easy to run a set of simulations with a range of parameters''.\\

We are designing this system for Ubicomp researchers\footnote{From now one we will be referring to a researcher as someone designing a mobile context-aware system based on the egocentric paradigm, which changes its behaviour based on how the body of the agent is in regards to the surrounding environment and entities.} and developers designing mobile context-aware systems that change their behaviour on how the body of the agent is in regards to the surrounding environment and entities. To offer the most to this target community, we would like the source code to be accessible for anyone who is interested, to provide feedback, to further develop it or to modify it in order to fit certain individual needs. These aspects are best covered by the open-source software model.\\

\input{Chapters/Chapter01/ProblemStatement}

\input{Chapters/Chapter01/Goal}

\input{Chapters/Chapter01/Method}

\input{Chapters/Chapter01/Scenario}

\input{Chapters/Chapter01/ThesisOverview}