%************************************************
\section{Monitoring Service} % (fold)
\label{sec:monitoring_service}
%************************************************
The purpose of the monitoring service component is to keep track of the objects marked with Ego metadata and classify them into SSM sets, as the simulation user controls the virtual agent to interact with the surrounding environment. In order to classify the objects, the service needs the following data: the agent's location and agent's orientation, the objects within the agent's field of vision and the Ego metadata of the monitored objects.\\

Next, we will briefly address how to determine whether objects are within the agent's filed of vision or not, followed by separate discussions on how the framework categorizes the objects into various SSM sets. The sets have been defined in Section \ref{subsec:ssm_params}.\\

%************************************************
\subsection{World Space}\label{subsec:world_space}
%************************************************
Central to all computations and decisions in this framework is the World Space. It is ought to contain all the objects in the environment carrying Ego metadata. To design for easy access to the objects carrying Ego metadata, when the simulation is started up, we parse the \ref{scene_graph} and each object carrying Ego metadata is added to the World Space. Whenever we need to categorize the monitored objects, we can simply refer to the World Space.\\

The drawback of this choice is that we are duplicating a set of data already present in the scene graph. The advantage is that we don't have to parse to \ref{scene_graph} every time we are interested in the objects carrying Ego metadata, accessing them from a reduced set of data brings performance improvements. Moreover, the World Space has to always be available through the API and it never changes, so computing it once, at the simulation's start-up, comes as a natural decision.
% subsection world_space (end)

%************************************************
\subsection{Objects within field of vision}\label{subsec:visible_objects}
%************************************************
To determine the objects within the agent's field of vision, we can rely on the agent's \ref{view_frustum} and the World Space \ref{subsec:world_space}. The frustum describes six planes: top, bottom, left, right, near and far. If the object is fully contained within or partially intersects the view frustum, we can consider the object is within the agent's field of vision. This is the same algorithm game engines use to determine whether to render or not a certain object (a process called culling).\\

The solution above does leave us one problem though: occluded objects will be considered visible. An object is considered to be occluded if other objects are in front of it blocking the agent's visibility over the target object.\\

To determine if an object is occluded, we have used \ref{ray_casting}. We are casting a ray from the agent's location (the agent's position vector) to the centre of the object (the object's position vector). If the ray intersects any other object before the target object, we consider it as being occluded, therefore not in the agent's field of vision.\\

This approach should work most of the time, but there are some edge cases with false positives. For example, a small object could be positioned in front of a really large object. Even if the centre point is occluded, the agent should still be able to perceive the object as it is large enough to recognize it based on all the details it provides. To overcome these kind of edge cases, we could cast a large number of rays to various points on the object and based on a more advanced algorithm, determine if the object is being occluded. We have decided to leave the design of a more advanced computation for future work, as the current solution is good enough to cover most of the cases, while developing a more advanced and correct algorithm needs more research in the field of 3D modelling and rendering.\\
% subsection objects_within_field_of_vision (end)

%************************************************
\subsection{Categorisation into SSM spaces}\label{subsec:categorisation_ssm_spaces}
%************************************************
We have addressed the World Space \ref{subsec:world_space} separately as most of the logic of the monitoring service revolves around it. We have addressed the reset of the SSM spaces in this section.\\

To determine whether an object is part of a particular SSM space or not, we need to know the distance to that object. The distance can be computed knowing the intersection of a ray cast from the agent's location to the target object. To avoid doing the same thing twice, this computation is part of the process where we determine if the object is within the agent's field of vision, as discussed in Section \ref{subsec:visible_objects}. There, we set a the distance in the Ego metadata as the \emph{LAST\_MEASURED\_DISTANCE} property.\\

Computing the distance as described above, brings up the same discussion we had in the previous Section \ref{subsec:visible_objects} about the occlusion of objects. It is based on the same algorithm so we compute the distance to the centre of the object. This can be improve in future work, along with the algorithm for detection of object occlusion.\\

Having all the necessary parameters, Perception Space, Recognition Set, Examinable Set and Action Space are computed as described in Section \ref{subsec:ssm_params}. We have used a less generic definition of these spaces than originally defined in \cite{pederson2011situative}, but future work could broaden them up to provide a classification closer to the ones defined in the theoretical framework. For example, an object in the Perception Space can be perceived as different entities depending on the distance to the agent. Given a hammer, from the furthest perception distance (X1) the agent would perceive it as an object; from a closer distance (X2), the agent would perceive it as an object certain kind of shape while from a more closer distance (X3),  the agent would perceive it as hammer. So, an object should be perceived in different ways based on the agent's proximity. Such improvements to the classification into the aforementioned sets, are target of the future work.\\

The Selected Set contains objects currently being physically manipulated. This happens when the agent picks an object up. In order for the agent to pick up a certain object, she/he must be targeting it (aim at it with the on screen arrow) and click the Left Mouse Button (pick up action); also, the object must be in the Action Space.\\

In the current design we only support pick-up/put-down as interaction with objects in the surrounding environment. Therefore, the only manipulation the agent is empowered to do is to pick objects up and move them around. That is why in the current work, the Manipulated Set completely overlaps the Selected Set.\\

Categorisation is triggered by the agent movements. Hence, every time the agent is moved or is looking around, using the controls described in Section \ref{sec:the_agent}, a new categorisation is triggered and the SSM sets are refreshed.\\
% subsection perception_space (end)

%************************************************
\subsection{Monitoring Service Summary}\label{subsec:monitoring_service_summary}
%************************************************
To summarize, in this section we described the design for object classification into SSM sets. It comes as a solution to requirement \ref{us:6}, classifying objects around the agent as the agent's field of vision changes and as the agent interacts with the environment.
% subsection subsec:monitoring_service_summary (end)

% section monitoring_service (end)