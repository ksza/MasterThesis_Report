%************************************************
\section{Future Work} % (fold)
\label{sec:future_work}
%************************************************
We have received a fair amount of feedback from our participants, as detailed in Appendix \ref{ch:evaluation_results}. We have collected all this user feedback into issues and enhancements in our source code repository\footnote{\url{https://github.com/ksza/EgoSim/issues}}. GitHub, the hosting service we use for our source code repository, offers the ''issues'' feature. This way issues can be managed on the same repository we keep our code in. Being open source, if anyone is willing to contribute to the project, immediate work items are already created and can be addressed.\\

In the remaining of this section we will detail the proposed future development for the EgoSim framework. Some of the proposed ideas were mentioned in previous sections of this thesis, others come from the user feedback.\\

%************************************************
\subsection{Interaction with virtual objects} 
%************************************************
We have taken the compromise to limit the current implementation to physical objects and devices. But to fully support the SSM context model, virtual objects must be classified as well. A virtual object can be a window displayed on display, or a song played on a mobile device. Interaction with these objects is done through mediators (devices).

%************************************************
\subsection{Third-person perspective} 
%************************************************
Avatar based games and simulations usually provide both first-person and third-person views, with an easy way to switch perspectives during runtime. For this project, a third-person perspective could be useful if we are to represent wearable devices. Also, it would the simulation user to observe the various body gestures the agent is doing. Moreover, in a third-person perspective the simulation user could get a better understanding of the agent's immediate surroundings.

%************************************************
\subsection{Improved objects detection} 
%************************************************
This goal is about improving the detection of objects in the agent's field of view. At the heart of our classification algorithm lies the way we determine if an object is currently visible to the agent or not. To do this, we cast a ray from the agent's current location to the centre point of the object's bounding box (think of it a the object's centre of gravity). If the ray intersects any other object before hitting the middle of the object, we consider the object as not being visible to the agent. This work in most cases, as the user feedback shows. But it has a lot of edge cases. For example the centre of a table can be occluded by a coin; this does not make the table unrecognisable, it just make our algorithm generate a false positive. Likewise, the agent could see through a hole in the wall the only centre of a drawer. Without other details, that object
might be unrecognisable.\\

One of the participants provided a very helpful feedback in this matter ''I consider the condition for an object to enter the perception space to be too weak. A user cannot identify an object if only a pixel is within his field of vision. Humans are built to recognize patterns, therefore a pattern-oriented metric could be used. For instance, the introduction of a diversity factor would help the simulation. If enough diversity of the object is within the perception space, it could trigger its presence in the perception space''. We entirely share this opinion and we will look into improving the classification algorithm as such.

%************************************************
\subsection{Improved computation of distance}
%************************************************
This goal is about improving the computation of distance to objects. When computing the distance to a certain object, we measure the distance from the agent's current location to the centre point of the object's bounding box. Therefore the distance might not always seem accurate because we are not taking into consideration the shape of the object. A possible solution would be to cast a ray from the agent's location to the centre of the object's bounding box and compute the distance to the closest intersection point (where the ray intersects the object). This way we could obtain a more accurate distance.

%************************************************
\subsection{A more granular perception of entities}
%************************************************
In the current implementation we have used a less generic definition of the SSM spaces. Future work could broaden them up to provide a classification closer to the ones defined in the theoretical framework. For example, an object in the Perception Space can be perceived as different entities depending on the distance to the agent. Given a hammer, from the furthest perception distance (X1) the agent would perceive it as an object; from a closer distance (X2), the agent would perceive it as an object certain kind of shape while from a more closer distance (X3), the agent would perceive it as hammer. So, an object should be perceived in different ways based on the agent's proximity.

%************************************************
\subsection{Improved API}
%************************************************
In this implementation to retrieve data from the API a third party service has to query the endpoint every time it needs updated data. This creates unnecessary overhead both for the framework and for the service. To improve this, we plan to look into alternatives, for instance an API based on publish/subscribe. With this communication pattern we would allow service to express their interest in contextual changes based on some rules they define. For example context changes of a certain entity with a given ID, context changes within a certain set, etc. Whenever one of the registered events occurs, the API would broadcast the event and every service registered for that type of event would receive the notification. With this pattern we can minimize the overhead on both the API's and the third party service's side. Moreover, the delivery of the notification would happen real-time, the service being able to take action at the same moment a certain context change has occurred.\\

We are also interested in evaluating if there would be value for third party service to dynamically manipulate some attribute through the API. For example, an external service might automatically turn up the light if the agent walks into a certain room.

%************************************************
\subsection{Improved ContextClient}
%************************************************
The advantage of having implemented the context client as a web page, is that no additional software installation is required. It can be accessed from any browser, on any platform. The problem with the current implementation is that it does not reflect the new information immediately after a change in the context occurs. It only does so every second. To improve refresh time, we though of using Web Sockets\footnote{\url{http://www.websocket.org}}. The WebSocket specification defines a full-duplex single socket connection over which messages can be sent between client and server. This means that instead of refreshing the context client every second, the ContextViewServer could push up to date information towards the context client, whenever available, through an open web socket connection.

%************************************************
\subsection{Improved context information} 
%************************************************
Many participants have pointed out that more context data should be monitored, besides visibility and proximity. To mention some of them: temperature, on/off state, orientation of certain objects (i.e. a computer screen might be close but you might be viewing it from behind), etc. We agree that for a complete context aware system some of these attributes have to be monitored by the framework. Another attribute useful context information we are interested in monitoring is the distance between objects. At the moment we only monitor the distance of objects relative to the agent's position. Knowing the distance between certain objects might be an important information for building business logic. Of course, adding more context information means to have it available through the API as well.\\

%************************************************
\subsection{Evaluate performance of the classification algorithm under heavy load} 
%************************************************
This goal is about evaluating how a very large number of tracked objects affect the performance of our classification algorithm. During the evaluation process we have built a couple of simulations using the EgoSim framework. Within a simulation we have tracked around 20 objects at a time. With this number of tracked objects, the simulations ran smoothly providing up to date SSM sets throughout the lifetime of the simulation. We plan on setting up a simulation where we monitor a large number of objects to see how that affects the performance of the classification algorithms, hence the simulation overall. We think there should not be much of a difference as we only take into account the objects currently in the agents field of vision which are usually constant in number through the lifetime of a simulation. But we leave this conclusion to be drawn once this research has been conducted in future work.

%************************************************
\subsection{Improved interaction between the agent and the environment}
%************************************************
We have received some interesting improvement ideas from the user feedback. To give a better feedback for the simulation user, we might display a relevant icon when the agent is targeting a certain objects which is in the ActionSpace to denote that the object can be interacted with. This enhancement could improve the simulation user's experience.\\

Next, we are interested designing a generic USE action when interacting with objects. Therefore, instead of taking a default action when the agent tries to interact with an objects, the simulation could display a list of actions available for that specific object; the list of actions would depend on the current context. For example, trying to interact with a mobile device could provide two actions: \emph{pick-up} and \emph{interact}. The \emph{pick-up} action would simply pick the object object up, while the \emph{interact} action would allow interacting with the virtual objects within the mobile device.\\

In this version of the framework the only default action we have provided is pick-up/drop-down. Some of the participants have mentioned other useful default actions like pull/push for drawers, open/close for doors, drop object at current location, turn on/off for switches, lamps, etc. All of these actions broaden the interactions the agent is enable do with the environment. We will plan on implementing these default actions.\\

In the current implementation we have used a deprecated CharacterControl\footnote{\url{http://hub.jmonkeyengine.org/javadoc/com/jme3/bullet/control/
CharacterControl.html}} class from the jMonkey framework to help us in with the agent's movement. As the CharacterControll class became deprecated, we plan on upgrading to the BetterCharacterControl\footnote{\url{http://hub.jmonkeyengine.org/javadoc/com/jme3/bullet/control/
BetterCharacterControl.html}} class. This will solve some of the agent navigation issues encountered during the evaluation process.

%************************************************
\subsection{Integration with a Virtual Reality (VR) kit}
%************************************************
This is an idea we have got from one of the participants. Integrating the simulation with a VR kit would create allow for maximum immersion of the simulation user into the simulated environment. On such VR kit is the Oculus Rift\footnote{\url{http://www.oculusvr.com/}} VR kit. The community of jMonkey Engine has already made some advances in integrating it with the Oculus Rift SDK. We are really interested looking into this aspect in future work, as we believe it has to offer a lot of benefits for how the simulation user experiences the simulated environment.

%************************************************
\subsection{Create an EgoSim IDE}
%************************************************
The jMonkey Engine SDK is free and open source. To create an integrated development experience for the system designer, we have the option extending the SDK and creating an EgoSim IDE of our own, making the configuration process easier and providing EgoSim specific design options. This would require a large amount of work. First because we need to dig deep into understanding how jMonkey Engine is implemented. Moreover, we need experience in developing plug-ins for the NetBeans platform, as the JME SDK is developed on top of NetBeans. At the moment this is not our biggest priority, but we are not excluding the possibility of looking into this aspect in the future.
% section sec:future_work (end)